{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark_version = '3.2.1'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12-3.2.1,spark-streaming-kafka-0-10_2.12:3.2.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create topic on kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka producer variables\n",
    "bootstrap_servers='kafka:9092'\n",
    "admin_client = AdminClient({\n",
    "    \"bootstrap.servers\": bootstrap_servers\n",
    "})\n",
    "topic_list = []\n",
    "topic_1='project_1'\n",
    "topic_list.append(NewTopic(topic_1, 1, 1))\n",
    "admin_client.create_topics(topic_list)\n",
    "\n",
    "p = Producer({'bootstrap.servers': bootstrap_servers})\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\" Called once for each message produced to indicate delivery result.\n",
    "        Triggered by poll() or flush(). \"\"\"\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {}'.format(msg.topic()))\n",
    "\n",
    "def confluent_kafka_producer(topic):\n",
    "    for data in simple_messages:\n",
    "        record_key = str(uuid.uuid4())\n",
    "        record_value = json.dumps({'data': data})\n",
    "        p.produce(topic, key=record_key, value=record_value, on_delivery=delivery_report)\n",
    "        p.poll(0)\n",
    "    p.flush()\n",
    "    print('we\\'ve sent {count} messages to {brokers}'.format(count=msg_count, brokers=bootstrap_servers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Producer message to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_messages = [\n",
    "'I love this pony',\n",
    "'This restaurant is great',\n",
    "'The weather is bad today',\n",
    "'I will go to the beach this weekend',\n",
    "'She likes to swim',\n",
    "'Apple is a great company'\n",
    "]\n",
    "msg_count=len(simple_messages)\n",
    "    \n",
    "confluent_kafka_producer(topic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkContext(master=\"spark://spark-master:7077\", appName=\"RealTimeML_topic_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SparkSession.builder \\\n",
    "    .getOrCreate()\n",
    "spark.getConf().getAll() == ss.sparkContext.getConf().getAll()\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"project_1\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# # Subscribe to 1 topic\n",
    "# df = ss \\\n",
    "#   .readStream \\\n",
    "#   .format('kafka') \\\n",
    "#   .option('kafka.bootstrap.servers', bootstrap_servers) \\\n",
    "#   .option(\"startingOffsets\", \"earliest\") \\\n",
    "#   .option('subscribe', topic_1) \\\n",
    "#   .load()\n",
    "\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df_raw.selectExpr('CAST(value AS STRING) as json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a small batch of data from kafka and display to the console\n",
    "\n",
    "schema = StructType([StructField('data', StringType())])\n",
    "\n",
    "df_json.select(from_json(df_json.json, schema).alias('raw_data')) \\\n",
    "  .select('raw_data.data') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test service\n",
    "import requests\n",
    "import json\n",
    "\n",
    "data_jsons = '{\"data\":\"' + simple_messages[1] + '\"}'\n",
    "print(data_jsons)\n",
    "result = requests.post('http://127.0.0.1:9000/predict', json=json.loads(data_jsons))\n",
    "print(json.dumps(result.json()))\n",
    "\n",
    "#vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sentiment_analysis(data):\n",
    "    import requests\n",
    "    import json\n",
    "    \n",
    "    result = requests.post('http://localhost:9000/predict', json=json.loads(data))\n",
    "    return json.dumps(result.json())\n",
    "\n",
    "vader_udf = udf(lambda data: apply_sentiment_analysis(data), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_input = StructType([StructField('data', StringType())])\n",
    "schema_output = StructType([StructField('neg', StringType()),\\\n",
    "                            StructField('pos', StringType()),\\\n",
    "                            StructField('neu', StringType()),\\\n",
    "                            StructField('compound', StringType())])\n",
    "\n",
    "df_json.select(from_json(df_json.json, schema_input).alias('sentence'),\\\n",
    "               from_json(vader_udf(df_json.json), schema_output).alias('response'))\\\n",
    "  .select('sentence.data', 'response.*') \\\n",
    "  .writeStream \\\n",
    "  .trigger(once=True) \\\n",
    "  .format(\"console\") \\\n",
    "  .start() \\\n",
    "  .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
