{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82102f1a-2c7c-4606-a70b-7c145637b290",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e417b-3374-4372-95c8-6c8fb1286bb2",
   "metadata": {},
   "source": [
    "- Tao đang kiểm tra nhẹ làm thế nào để run một chiếc spark load tensorflow model nha mấy bé. \n",
    "\n",
    "- mấy bé thấy hay thì nhớ lài lái lai và sè sé se. và follow github cho tao nha: [Github lè](https://github.com/dnguyenngoc) or [Github group lè](https://github.com/apot-group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da611d6-3e1e-435e-a4ce-4d52a67679e5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 1:</b> Import Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0793a226-5c5e-4218-a324-e32c62400097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.kafka:kafka-clients:3.1.0,org.apache.spark:spark-core_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.spark:spark-streaming_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.hadoop:hadoop-core:1.2.1' \\\n",
    "                                    + ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7cbeb-7606-4091-af60-dd14dc805ff1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 2:</b> Start Spark Session\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af09b388-3f62-422b-8f99-0aff4c2020e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://24897eca24e2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[1] appName=test>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss  = SparkSession.builder.master('local[1]') \\\n",
    "                  .appName(\"test\") \\\n",
    "                  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                  .config(\"spark.executor.cores\", \"2\") \\\n",
    "                  .config(\"spark.driver.cores\", \"2\") \\\n",
    "                  .getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a4106-99e2-4c67-80b9-6885f4b18bfa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 3:</b> Load tensoflow model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6195c46-d56e-444d-b90c-0b458167e02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "      <th>G4</th>\n",
       "      <th>G5</th>\n",
       "      <th>G6</th>\n",
       "      <th>G7</th>\n",
       "      <th>G8</th>\n",
       "      <th>G9</th>\n",
       "      <th>G10</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22901</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22902</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22903</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22904</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22905 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       G1  G2  G3  G4  G5  G6  G7  G8  G9  G10  label\n",
       "0       0   0   0   1   0   0   0   0   0    0      0\n",
       "1       0   0   0   0   1   0   0   0   0    0      0\n",
       "2       0   0   1   0   0   0   0   0   0    0      0\n",
       "3       0   0   1   0   0   1   1   0   0    1      1\n",
       "4       0   0   1   0   1   1   0   1   1    0      1\n",
       "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ...    ...\n",
       "22900   0   0   1   0   0   0   0   0   0    0      0\n",
       "22901   1   0   0   0   0   0   0   0   0    0      0\n",
       "22902   0   0   0   0   0   0   0   0   0    0      1\n",
       "22903   0   0   1   0   0   0   0   0   0    0      0\n",
       "22904   0   0   0   0   0   0   0   0   0    0      0\n",
       "\n",
       "[22905 rows x 11 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.read_csv(\n",
    "     \"https://github.com/bgweber/Twitch/raw/master/Recommendations/games-expand.csv\")\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df52bad7-7dbf-403d-98b2-4b484b7f367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 16:27:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+-----+-------+------------+\n",
      "| G1| G2| G3| G4| G5| G6| G7| G8| G9|G10|label|user_id|partition_id|\n",
      "+---+---+---+---+---+---+---+---+---+---+-----+-------+------------+\n",
      "|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|    0|      1|           1|\n",
      "|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|    0|      2|           2|\n",
      "|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|    0|      3|           3|\n",
      "|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|    0|      4|           4|\n",
      "|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|    0|      5|           5|\n",
      "|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|    0|      6|           6|\n",
      "|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|    0|      7|           7|\n",
      "|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|    0|      8|           8|\n",
      "|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|    0|      9|           9|\n",
      "|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|    1|     10|           0|\n",
      "|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|    1|     11|           1|\n",
      "|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|    0|     12|           2|\n",
      "|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|    0|     13|           3|\n",
      "|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|    0|     14|           4|\n",
      "|  1|  1|  0|  0|  0|  1|  1|  0|  0|  0|    0|     15|           5|\n",
      "|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|    0|     16|           6|\n",
      "|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|    0|     17|           7|\n",
      "|  1|  1|  0|  1|  1|  1|  1|  1|  0|  0|    0|     18|           8|\n",
      "|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|    0|     19|           9|\n",
      "|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|    0|     20|           0|\n",
      "+---+---+---+---+---+---+---+---+---+---+-----+-------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = ss.createDataFrame(pandas_df)\n",
    "\n",
    "# assign a user ID and a partition ID using Spark SQL\n",
    "spark_df.createOrReplaceTempView(\"spark_df\")\n",
    "spark_df = ss.sql(\"\"\"\n",
    "select *, user_id%10 as partition_id \n",
    "from (\n",
    "  select *, row_number() over (order by rand()) as user_id\n",
    "  from spark_df\n",
    ") \n",
    "\"\"\")\n",
    "\n",
    "# preview the results\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16d78d8a-9768-4742-94b3-e35a2251d43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df = spark_df.toPandas().drop(['user_id', 'partition_id'], axis = 1)\n",
    "\n",
    "y_train = df['label']\n",
    "x_train = df.drop(['label'], axis=1)\n",
    "\n",
    "# use logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c13deb2-01b2-44ed-9472-2c48abe54cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|          prediction|\n",
      "+-------+--------------------+\n",
      "|      1| 0.06414947633646334|\n",
      "|      2| 0.14233082944037895|\n",
      "|      3|0.045371032095244526|\n",
      "|      4| 0.07175760020956665|\n",
      "|      5|  0.1350842735918626|\n",
      "|      6| 0.08760914967461457|\n",
      "|      7| 0.21250043631356066|\n",
      "|      8| 0.07175760020956665|\n",
      "|      9| 0.10030953139889025|\n",
      "|     10|0.045371032095244526|\n",
      "|     11| 0.10030953139889025|\n",
      "|     12|  0.1350842735918626|\n",
      "|     13|  0.3809000596496006|\n",
      "|     14|  0.1350842735918626|\n",
      "|     15| 0.05527800275089643|\n",
      "|     16| 0.06414947633646334|\n",
      "|     17| 0.06414947633646334|\n",
      "|     18| 0.03229025205257302|\n",
      "|     19|0.045599767602852205|\n",
      "|     20| 0.07175760020956665|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pull all data to the driver node\n",
    "sample_df = spark_df.toPandas()\n",
    "\n",
    "# create a prediction for each user \n",
    "ids = sample_df['user_id']\n",
    "x_train = sample_df.drop(['label', 'user_id', 'partition_id'], axis=1)\n",
    "pred = model.predict_proba(x_train)\n",
    "result_df = pd.DataFrame({'user_id': ids, 'prediction': pred[:,1]})\n",
    "\n",
    "# display the results \n",
    "e = ss.createDataFrame(result_df)\n",
    "e.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8837348-f916-4c2d-be17-f0a32ff56ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.format(\"image\").load('/work/data/images/').limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "784bd5fa-7e3e-4fc3-8784-1596465d4e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "22/03/02 16:36:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|          prediction|\n",
      "+-------+--------------------+\n",
      "|      1| 0.06414947633646334|\n",
      "|      2| 0.14233082944037895|\n",
      "|      3|0.045371032095244526|\n",
      "|      4| 0.07175760020956665|\n",
      "|      5|  0.1350842735918626|\n",
      "|      6| 0.08760914967461457|\n",
      "|      7| 0.21250043631356066|\n",
      "|      8| 0.07175760020956665|\n",
      "|      9| 0.10030953139889025|\n",
      "|     10|0.045371032095244526|\n",
      "|     11| 0.10030953139889025|\n",
      "|     12|  0.1350842735918626|\n",
      "|     13|  0.3809000596496006|\n",
      "|     14|  0.1350842735918626|\n",
      "|     15| 0.05527800275089643|\n",
      "|     16| 0.06414947633646334|\n",
      "|     17| 0.06414947633646334|\n",
      "|     18| 0.03229025205257302|\n",
      "|     19|0.045599767602852205|\n",
      "|     20| 0.07175760020956665|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# define a schema for the result set, the user ID and model prediction\n",
    "schema = StructType([StructField('user_id', LongType(), True),\n",
    "                     StructField('prediction', DoubleType(), True)])  \n",
    "\n",
    "# define the Pandas UDF \n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def apply_model(sample_pd):\n",
    "\n",
    "    # run the model on the partitioned data set \n",
    "    ids = sample_df['user_id']\n",
    "    x_train = sample_df.drop(['label', 'user_id', 'partition_id'], axis=1)\n",
    "    pred = model.predict_proba(x_train)\n",
    "\n",
    "    return pd.DataFrame({'user_id': ids, 'prediction': pred[:,1]})\n",
    "  \n",
    "# partition the data and run the UDF  \n",
    "results = spark_df.groupby('partition_id').apply(apply_model)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa9ca5-6ab9-4560-8d1a-340672b89ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546be87-bad6-4455-8de3-99400ca686bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
