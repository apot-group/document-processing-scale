{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc90a810-90c2-42fd-a080-185477df0c01",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f97d8-33b5-4713-a8db-388fbb27d57e",
   "metadata": {},
   "source": [
    "Tao đang muốn test run full workflow deep learning trên spark thông qua py-spark cell. deepleaning với thư viện tensorflow, keras. nội dung thực hiện bao gồm load một csv file bank gồm:\n",
    "[id,age,job,marital,education,default,balance,housing,loan,contact,day,month,duration,campaign,pdays,previous,poutcome,deposit]\n",
    "load và review, sau đó drop some field sau đó làm thêm một số cái linh sờ tinh như extract feature transform bla bla chung quy là test spark pineline. Hồi sau sẽ thêm nhiều cái như thay vì load từ csv sẽ load từ chanel trong kafka streaming và read file từ hdfs. bla bla =))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02a96d-aee2-472f-a530-bec37b051c2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 1:</b> Import Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6fa495-5dd7-4f13-81c0-3aa4a5de1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.kafka:kafka-clients:3.1.0,org.apache.spark:spark-core_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.spark:spark-streaming_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.hadoop:hadoop-core:1.2.1' \\\n",
    "#                                     + ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b6d9d-75a8-46d8-adec-1e290c7e7754",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 2:</b> Start Spark Session\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65a0875-ea5f-449f-923a-cb1b4d1af2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/02 04:35:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5a36dd2aff00:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8803c2df40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOPIC = 'kafka-sample'\n",
    "SPARK_SERVER=\"spark://spark-master:7077\"\n",
    "\n",
    "ss  = SparkSession.builder.master('local[1]') \\\n",
    "                  .appName(\"test\") \\\n",
    "                  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                  .config(\"spark.executor.cores\", \"2\") \\\n",
    "                  .config(\"spark.driver.cores\", \"2\") \\\n",
    "                  .getOrCreate()\n",
    "ss\n",
    "# sc = ss.sparkContext\n",
    "# sql_c = SQLContext(sc)\n",
    "# sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a70cc-0a99-4460-ae6d-a1dc2ef07369",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 3:</b> Load and Preview Data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326fc08-7317-4604-99f5-e1ffc0fc997c",
   "metadata": {},
   "source": [
    "Tại đây, tao sẽ tải dữ liệu. Dữ liệu tao sử dụng đến từ một cuộc thi Kaggle [Kaggle competition](https://www.kaggle.com/janiobachmann/bank-marketing-dataset). Đó là một tập dữ liệu ngân hàng điển hình. Ở đây tao sử dụng tham số invSchema giúp xác định các loại tính năng khi tải dữ liệu. Theo tài liệu PySpark [PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html), điều này \"yêu cầu thêm một lần chuyển dữ liệu\". Vì dữ liệu ngân hàng mà tao đang tải chỉ có ~ 11 nghìn lần quan sát nên không mất nhiều thời gian, nhưng có thể đáng chú ý nếu mày có một tập dữ liệu rất lớn.\n",
    "\n",
    "Sau khi tải dữ liệu, chúng tao có thể thấy lược đồ và các loại tính năng khác nhau. Tất cả các tính năng của chúng tao là kiểu chuỗi hoặc số nguyên. Sau đó, chúng tao xem trước 5 quan sát đầu tiên. tao khá quen thuộc với thư viện Pandas python nên qua ví dụ này, mày sẽ thấy tao sử dụng toPandas () để chuyển đổi khung dữ liệu spark thành khung dữ liệu gấu trúc và thực hiện một số thao tác. Không đúng không sai, chỉ là dễ dàng hơn cho tao.\n",
    "\n",
    "Cuối cùng, chúng tao sẽ bỏ 2 cột ngày vì chúng tao sẽ không sử dụng các cột đó trong mô hình học sâu của mình. Chúng có thể rất quan trọng và được kỳ công bởi tao quyết định thả tất cả chúng lại với nhau. ồ tao đc kết quả như mong muốn lè =)))\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "tại đây tao gặp một vấn đề chưa giải quyết đc là chưa dùng spark để write file tới hdfs đc. méo hiểu kiểu gì lun ...(-.-)! maybe là tao sẽ kiểm tra nó sau. nên hiện tại tao dùng python viết một file tới hdfs trước nha mấy thằng mặt lồng. sau hồi test thì tau phát hiện ra không những không write đc đến hdfs mà còn đọc méo đc lun. éo hiểu kiểu gì?\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    1. hiện tại tao chưa tìm đc giải pháp nên tao tạm thời tạo pyspark df from df normal nha.!\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc6bcdc-6bd7-4020-965b-04970b7792c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, age: string, job: string, marital: string, education: string, default: string, balance: string, housing: string, loan: string, contact: string, day: string, month: string, duration: string, campaign: string, pdays: string, previous: string, poutcome: string, deposit: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ss.read.csv(path=\"/work/data/bank.csv\", sep=\",\", header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2ea264-bf7d-481a-9418-e1f6468d0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write file bank.csv to hdfs\n",
    "# from hdfs import InsecureClient\n",
    "# client = InsecureClient('http://namenode:9870')\n",
    "# with client.write('/bank.csv', encoding = 'utf-8') as writer:\n",
    "#     df.to_csv(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ba2c940-eddb-4d83-bdf7-83e9340f2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- campaign: string (nullable = true)\n",
      " |-- pdays: string (nullable = true)\n",
      " |-- previous: string (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11162"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data to Spark Dataframe\n",
    "print(df.printSchema())\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159e5fdb-3ffc-49f8-8b68-22378f89e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnessary Featur/work/Day and Month)\n",
    "df = df.drop('day', 'month')\n",
    "# ss.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d57d7ff-840c-47c5-b2e8-68f15e0f005d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id age         job  marital  education default balance housing loan  \\\n",
       "0  1  59      admin.  married  secondary      no    2343     yes   no   \n",
       "1  2  56      admin.  married  secondary      no      45      no   no   \n",
       "2  3  41  technician  married  secondary      no    1270     yes   no   \n",
       "3  4  55    services  married  secondary      no    2476     yes   no   \n",
       "4  5  54      admin.  married   tertiary      no     184      no   no   \n",
       "\n",
       "   contact duration campaign pdays previous poutcome deposit  \n",
       "0  unknown     1042        1    -1        0  unknown     yes  \n",
       "1  unknown     1467        1    -1        0  unknown     yes  \n",
       "2  unknown     1389        1    -1        0  unknown     yes  \n",
       "3  unknown      579        1    -1        0  unknown     yes  \n",
       "4  unknown      673        2    -1        0  unknown     yes  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(10).toPandas().head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517cd19-a381-4a78-b459-688c40ac51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to select features to scale given their skew\n",
    "def select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['']):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660804a5-88e3-4127-9038-a8161d11b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = ss.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc011475-6549-4165-a7bb-25fae4425c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ab33d-e7a0-4f3b-b68d-ec04f28cb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df3e78-0087-4240-9c90-da1aaa6b8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa7af7-5f7a-4779-98e7-5c3730850e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
