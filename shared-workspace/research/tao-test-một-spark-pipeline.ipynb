{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc90a810-90c2-42fd-a080-185477df0c01",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f97d8-33b5-4713-a8db-388fbb27d57e",
   "metadata": {},
   "source": [
    "Tao đang muốn test run full workflow deep learning trên spark thông qua py-spark cell. deepleaning với thư viện tensorflow, keras. nội dung thực hiện bao gồm load một csv file bank gồm:\n",
    "[id,age,job,marital,education,default,balance,housing,loan,contact,day,month,duration,campaign,pdays,previous,poutcome,deposit]\n",
    "load và review, sau đó drop some field sau đó làm thêm một số cái linh sờ tinh như extract feature transform bla bla chung quy là test spark pineline. Hồi sau sẽ thêm nhiều cái như thay vì load từ csv sẽ load từ chanel trong kafka streaming và read file từ hdfs. bla bla =))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02a96d-aee2-472f-a530-bec37b051c2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 1:</b> Import Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fa495-5dd7-4f13-81c0-3aa4a5de1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.kafka:kafka-clients:3.1.0,org.apache.spark:spark-core_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.spark:spark-streaming_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.0.2,' \\\n",
    "#                                     + 'org.apache.hadoop:hadoop-core:1.2.1' \\\n",
    "#                                     + ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b6d9d-75a8-46d8-adec-1e290c7e7754",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 2:</b> Start Spark Session\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a0875-ea5f-449f-923a-cb1b4d1af2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = 'kafka-sample'\n",
    "SPARK_SERVER=\"spark://spark-master:7077\"\n",
    "\n",
    "ss  = SparkSession.builder.master('local[2]') \\\n",
    "                  .appName(\"test\") \\\n",
    "                  .config(\"spark.executor.memory\", \"2g\") \\\n",
    "                  .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                  .config(\"spark.executor.cores\", \"2\") \\\n",
    "                  .config(\"spark.driver.cores\", \"2\") \\\n",
    "                  .getOrCreate()\n",
    "ss\n",
    "# sc = ss.sparkContext\n",
    "# sql_c = SQLContext(sc)\n",
    "# sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a70cc-0a99-4460-ae6d-a1dc2ef07369",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 3:</b> Load and Preview Data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326fc08-7317-4604-99f5-e1ffc0fc997c",
   "metadata": {},
   "source": [
    "Tại đây, tao sẽ tải dữ liệu. Dữ liệu tao sử dụng đến từ một cuộc thi Kaggle [Kaggle competition](https://www.kaggle.com/janiobachmann/bank-marketing-dataset). Đó là một tập dữ liệu ngân hàng điển hình. Ở đây tao sử dụng tham số invSchema giúp xác định các loại tính năng khi tải dữ liệu. Theo tài liệu PySpark [PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html), điều này \"yêu cầu thêm một lần chuyển dữ liệu\". Vì dữ liệu ngân hàng mà tao đang tải chỉ có ~ 11 nghìn lần quan sát nên không mất nhiều thời gian, nhưng có thể đáng chú ý nếu mày có một tập dữ liệu rất lớn.\n",
    "\n",
    "Sau khi tải dữ liệu, chúng tao có thể thấy lược đồ và các loại tính năng khác nhau. Tất cả các tính năng của chúng tao là kiểu chuỗi hoặc số nguyên. Sau đó, chúng tao xem trước 5 quan sát đầu tiên. tao khá quen thuộc với thư viện Pandas python nên qua ví dụ này, mày sẽ thấy tao sử dụng toPandas () để chuyển đổi khung dữ liệu spark thành khung dữ liệu gấu trúc và thực hiện một số thao tác. Không đúng không sai, chỉ là dễ dàng hơn cho tao.\n",
    "\n",
    "Cuối cùng, chúng tao sẽ bỏ 2 cột ngày vì chúng tao sẽ không sử dụng các cột đó trong mô hình học sâu của mình. Chúng có thể rất quan trọng và được kỳ công bởi tao quyết định thả tất cả chúng lại với nhau. ồ tao đc kết quả như mong muốn lè =)))\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "tại đây tao gặp một vấn đề chưa giải quyết đc là chưa dùng spark để write file tới hdfs đc. méo hiểu kiểu gì lun ...(-.-)! maybe là tao sẽ kiểm tra nó sau. nên hiện tại tao dùng python viết một file tới hdfs trước nha mấy thằng mặt lồng. sau hồi test thì tau phát hiện ra không những không write đc đến hdfs mà còn đọc méo đc lun. éo hiểu kiểu gì?\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    1. hiện tại tao chưa tìm đc giải pháp nên tao tạm thời tạo pyspark df from df normal nha.!\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6bcdc-6bd7-4020-965b-04970b7792c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ss.read.csv(path=\"/work/data/bank.csv\", sep=\",\", header=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ea264-bf7d-481a-9418-e1f6468d0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write file bank.csv to hdfs\n",
    "# from hdfs import InsecureClient\n",
    "# client = InsecureClient('http://namenode:9870')\n",
    "# with client.write('/bank.csv', encoding = 'utf-8') as writer:\n",
    "#     df.to_csv(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2c940-eddb-4d83-bdf7-83e9340f2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data to Spark Dataframe\n",
    "print(df.printSchema())\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e5fdb-3ffc-49f8-8b68-22378f89e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnessary Featur/work/Day and Month)\n",
    "df = df.drop('day', 'month')\n",
    "# ss.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57d7ff-840c-47c5-b2e8-68f15e0f005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(10).toPandas().head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517cd19-a381-4a78-b459-688c40ac51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to select features to scale given their skew\n",
    "def select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['']):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660804a5-88e3-4127-9038-a8161d11b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = ss.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc011475-6549-4165-a7bb-25fae4425c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ab33d-e7a0-4f3b-b68d-ec04f28cb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df3e78-0087-4240-9c90-da1aaa6b8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa7af7-5f7a-4779-98e7-5c3730850e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
