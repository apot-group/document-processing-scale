version: '3.8'
services:
  # Spark master
  spark-master:
    image: duynguyenngoc/dps-hadoop-spark:3.3.1
    container_name: spark-master
    restart: unless-stopped
    volumes:
      - ./dps-spark/apps/:/opt/spark-apps
      - ./dps-spark/data/:/opt/spark-data
      - shared-workspace:/opt/workspace
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_WORKLOAD=master
    networks:
     - dp-net
  
  # Spark worker
  spark-worker-a:
    image: duynguyenngoc/dps-hadoop-spark:3.3.1
    ports:
      - "8081:8080"
      - "7001:7000"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2G
      - SPARK_DRIVER_MEMORY=2G
      - SPARK_EXECUTOR_MEMORY=2G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker-a
    volumes:
      - ./dps-spark/apps/:/opt/spark-apps
      - ./dps-spark/data/:/opt/spark-data
      - shared-workspace:/opt/workspace
    networks:
      - dp-net

  # Spark jupyerlab
  dps-jypyter:
    image: duynguyenngoc/dps-jupyter-lab:3.3.1
    container_name: dps-jupyter
    restart: unless-stopped
    volumes:
      - ./dps-jupyter/work/:/work/
      - shared-workspace:/opt/workspace
    ports:
      - 8888:8888
    networks:
     - dp-net

volumes:
  # Share volume
  shared-workspace:
    name: "hadoop-distributed-file-system"
    driver: local
networks:
  dp-net: