{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc90a810-90c2-42fd-a080-185477df0c01",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f97d8-33b5-4713-a8db-388fbb27d57e",
   "metadata": {},
   "source": [
    "Tao đang muốn test run full workflow deep learning trên spark thông qua py-spark cell. deepleaning với thư viện tensorflow, keras. nội dung thực hiện bao gồm load một csv file bank gồm:\n",
    "[id,age,job,marital,education,default,balance,housing,loan,contact,day,month,duration,campaign,pdays,previous,poutcome,deposit]\n",
    "load và review, sau đó drop some field sau đó làm thêm một số cái linh sờ tinh như extract feature transform bla bla chung quy là test spark pineline. Hồi sau sẽ thêm nhiều cái như thay vì load từ csv sẽ load từ chanel trong kafka streaming và read file từ hdfs. bla bla =))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02a96d-aee2-472f-a530-bec37b051c2a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 1:</b> Import Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6fa495-5dd7-4f13-81c0-3aa4a5de1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.kafka:kafka-clients:3.1.0,org.apache.spark:spark-core_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.spark:spark-streaming_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.spark:spark-token-provider-kafka-0-10_2.12:3.0.2,' \\\n",
    "                                    + 'org.apache.hadoop:hadoop-core:1.2.1' \\\n",
    "                                    + ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b6d9d-75a8-46d8-adec-1e290c7e7754",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 2:</b> Start Spark Session\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65a0875-ea5f-449f-923a-cb1b4d1af2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3e91b9e065d0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=test>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOPIC = 'kafka-sample'\n",
    "SPARK_SERVER=\"spark://spark-master:7077\"\n",
    "\n",
    "ss  = SparkSession.builder.master(SPARK_SERVER) \\\n",
    "                  .appName(\"test\") \\\n",
    "                  .config(\"spark.executor.memory\", \"6g\") \\\n",
    "                  .config(\"spark.driver.memory\", \"6g\") \\\n",
    "                  .config(\"spark.executor.cores\", \"6\") \\\n",
    "                  .config(\"spark.driver.cores\", \"6\") \\\n",
    "                  .getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "sql_c = SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a70cc-0a99-4460-ae6d-a1dc2ef07369",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 3:</b> Load and Preview Data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326fc08-7317-4604-99f5-e1ffc0fc997c",
   "metadata": {},
   "source": [
    "Tại đây, tao sẽ tải dữ liệu. Dữ liệu tao sử dụng đến từ một cuộc thi Kaggle [Kaggle competition](https://www.kaggle.com/janiobachmann/bank-marketing-dataset). Đó là một tập dữ liệu ngân hàng điển hình. Ở đây tao sử dụng tham số invSchema giúp xác định các loại tính năng khi tải dữ liệu. Theo tài liệu PySpark [PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html), điều này \"yêu cầu thêm một lần chuyển dữ liệu\". Vì dữ liệu ngân hàng mà tao đang tải chỉ có ~ 11 nghìn lần quan sát nên không mất nhiều thời gian, nhưng có thể đáng chú ý nếu mày có một tập dữ liệu rất lớn.\n",
    "\n",
    "Sau khi tải dữ liệu, chúng tao có thể thấy lược đồ và các loại tính năng khác nhau. Tất cả các tính năng của chúng tao là kiểu chuỗi hoặc số nguyên. Sau đó, chúng tao xem trước 5 quan sát đầu tiên. tao khá quen thuộc với thư viện Pandas python nên qua ví dụ này, mày sẽ thấy tao sử dụng toPandas () để chuyển đổi khung dữ liệu spark thành khung dữ liệu gấu trúc và thực hiện một số thao tác. Không đúng không sai, chỉ là dễ dàng hơn cho tao.\n",
    "\n",
    "Cuối cùng, chúng tao sẽ bỏ 2 cột ngày vì chúng tao sẽ không sử dụng các cột đó trong mô hình học sâu của mình. Chúng có thể rất quan trọng và được kỳ công bởi tao quyết định thả tất cả chúng lại với nhau. ồ tao đc kết quả như mong muốn lè =)))\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "tại đây tao gặp một vấn đề chưa giải quyết đc là chưa dùng spark để write file tới hdfs đc. méo hiểu kiểu gì lun ...(-.-)! maybe là tao sẽ kiểm tra nó sau. nên hiện tại tao dùng python viết một file tới hdfs trước nha mấy thằng mặt lồng. sau hồi test thì tau phát hiện ra không những không write đc đến hdfs mà còn đọc méo đc lun. éo hiểu kiểu gì?\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    1. hiện tại tao chưa tìm đc giải pháp nên tao tạm thời tạo pyspark df from df normal nha.!\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc6bcdc-6bd7-4020-965b-04970b7792c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11157</th>\n",
       "      <td>11158</td>\n",
       "      <td>33</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>single</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>20</td>\n",
       "      <td>apr</td>\n",
       "      <td>257</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11158</th>\n",
       "      <td>11159</td>\n",
       "      <td>39</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>733</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16</td>\n",
       "      <td>jun</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11159</th>\n",
       "      <td>11160</td>\n",
       "      <td>32</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>aug</td>\n",
       "      <td>156</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11160</th>\n",
       "      <td>11161</td>\n",
       "      <td>43</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>8</td>\n",
       "      <td>may</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>5</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11161</th>\n",
       "      <td>11162</td>\n",
       "      <td>34</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>9</td>\n",
       "      <td>jul</td>\n",
       "      <td>628</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11162 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  age          job  marital  education default  balance housing  \\\n",
       "0          1   59       admin.  married  secondary      no     2343     yes   \n",
       "1          2   56       admin.  married  secondary      no       45      no   \n",
       "2          3   41   technician  married  secondary      no     1270     yes   \n",
       "3          4   55     services  married  secondary      no     2476     yes   \n",
       "4          5   54       admin.  married   tertiary      no      184      no   \n",
       "...      ...  ...          ...      ...        ...     ...      ...     ...   \n",
       "11157  11158   33  blue-collar   single    primary      no        1     yes   \n",
       "11158  11159   39     services  married  secondary      no      733      no   \n",
       "11159  11160   32   technician   single  secondary      no       29      no   \n",
       "11160  11161   43   technician  married  secondary      no        0      no   \n",
       "11161  11162   34   technician  married  secondary      no        0      no   \n",
       "\n",
       "      loan   contact  day month  duration  campaign  pdays  previous poutcome  \\\n",
       "0       no   unknown    5   may      1042         1     -1         0  unknown   \n",
       "1       no   unknown    5   may      1467         1     -1         0  unknown   \n",
       "2       no   unknown    5   may      1389         1     -1         0  unknown   \n",
       "3       no   unknown    5   may       579         1     -1         0  unknown   \n",
       "4       no   unknown    5   may       673         2     -1         0  unknown   \n",
       "...    ...       ...  ...   ...       ...       ...    ...       ...      ...   \n",
       "11157   no  cellular   20   apr       257         1     -1         0  unknown   \n",
       "11158   no   unknown   16   jun        83         4     -1         0  unknown   \n",
       "11159   no  cellular   19   aug       156         2     -1         0  unknown   \n",
       "11160  yes  cellular    8   may         9         2    172         5  failure   \n",
       "11161   no  cellular    9   jul       628         1     -1         0  unknown   \n",
       "\n",
       "      deposit  \n",
       "0         yes  \n",
       "1         yes  \n",
       "2         yes  \n",
       "3         yes  \n",
       "4         yes  \n",
       "...       ...  \n",
       "11157      no  \n",
       "11158      no  \n",
       "11159      no  \n",
       "11160      no  \n",
       "11161      no  \n",
       "\n",
       "[11162 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file from csv like dataframe\n",
    "df = pd.read_csv('./bank.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2ea264-bf7d-481a-9418-e1f6468d0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write file bank.csv to hdfs\n",
    "# from hdfs import InsecureClient\n",
    "# client = InsecureClient('http://namenode:9870')\n",
    "# with client.write('/bank.csv', encoding = 'utf-8') as writer:\n",
    "#     df.to_csv(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba2c940-eddb-4d83-bdf7-83e9340f2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: long (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- campaign: long (nullable = true)\n",
      " |-- pdays: long (nullable = true)\n",
      " |-- previous: long (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load Data to Spark Dataframe\n",
    "df_s = ss.createDataFrame(df)\n",
    "print(df_s.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159e5fdb-3ffc-49f8-8b68-22378f89e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnessary Features (Day and Month)\n",
    "df_s = df_s.drop('day', 'month')\n",
    "ss.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d57d7ff-840c-47c5-b2e8-68f15e0f005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_s\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:157\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    158\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    160\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_s.limit(10).toPandas().head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5517cd19-a381-4a78-b459-688c40ac51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to select features to scale given their skew\n",
    "def select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['']):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660804a5-88e3-4127-9038-a8161d11b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = ss.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc011475-6549-4165-a7bb-25fae4425c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ab33d-e7a0-4f3b-b68d-ec04f28cb0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df3e78-0087-4240-9c90-da1aaa6b8716",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa7af7-5f7a-4779-98e7-5c3730850e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
