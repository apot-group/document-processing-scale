[2022-02-28 06:41:01,113] {taskinstance.py:1037} INFO - Dependencies all met for <TaskInstance: 0747_pipeline.document_etl manual__2022-02-28T06:37:06.521999+00:00 [queued]>
[2022-02-28 06:41:01,133] {taskinstance.py:1037} INFO - Dependencies all met for <TaskInstance: 0747_pipeline.document_etl manual__2022-02-28T06:37:06.521999+00:00 [queued]>
[2022-02-28 06:41:01,134] {taskinstance.py:1243} INFO - 
--------------------------------------------------------------------------------
[2022-02-28 06:41:01,134] {taskinstance.py:1244} INFO - Starting attempt 3 of 3
[2022-02-28 06:41:01,135] {taskinstance.py:1245} INFO - 
--------------------------------------------------------------------------------
[2022-02-28 06:41:01,191] {taskinstance.py:1264} INFO - Executing <Task(SparkSubmitOperator): document_etl> on 2022-02-28 06:37:06.521999+00:00
[2022-02-28 06:41:01,193] {standard_task_runner.py:52} INFO - Started process 205 to run task
[2022-02-28 06:41:01,196] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', '0747_pipeline', 'document_etl', 'manual__2022-02-28T06:37:06.521999+00:00', '--job-id', '73', '--raw', '--subdir', 'DAGS_FOLDER/0747.py', '--cfg-path', '/tmp/tmp8_3gxkdb', '--error-file', '/tmp/tmpz4qzt_rx']
[2022-02-28 06:41:01,198] {standard_task_runner.py:77} INFO - Job 73: Subtask document_etl
[2022-02-28 06:41:01,298] {logging_mixin.py:109} INFO - Running <TaskInstance: 0747_pipeline.document_etl manual__2022-02-28T06:37:06.521999+00:00 [running]> on host c5f5c4f93487
[2022-02-28 06:41:01,397] {taskinstance.py:1431} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=0747_pipeline
AIRFLOW_CTX_TASK_ID=document_etl
AIRFLOW_CTX_EXECUTION_DATE=2022-02-28T06:37:06.521999+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-28T06:37:06.521999+00:00
[2022-02-28 06:41:01,407] {base.py:79} INFO - Using connection to: id: spark_default. Host: spark://spark-master, Port: 7077, Schema: , Login: , Password: None, extra: {'queue': 'default', 'spark-home': '', 'spark-binary': 'spark-submit', 'namespace': 'default'}
[2022-02-28 06:41:01,408] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --name 0747 Pipeline --queue default /sparks/production/0747/document_etl.py
[2022-02-28 06:41:02,807] {spark_submit.py:488} INFO - WARNING: An illegal reflective access operation has occurred
[2022-02-28 06:41:02,808] {spark_submit.py:488} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-02-28 06:41:02,809] {spark_submit.py:488} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-02-28 06:41:02,809] {spark_submit.py:488} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-02-28 06:41:02,810] {spark_submit.py:488} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-02-28 06:41:03,294] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:41:03,295] {spark_submit.py:488} INFO - Step 1: Import Libraries
[2022-02-28 06:41:03,296] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:41:03,546] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:41:03,547] {spark_submit.py:488} INFO - Step 2: Start Spark Session
[2022-02-28 06:41:03,548] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:41:03,653] {spark_submit.py:488} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-02-28 06:41:03,665] {spark_submit.py:488} INFO - 22/02/28 06:41:03 INFO SparkContext: Running Spark version 3.2.1
[2022-02-28 06:41:03,788] {spark_submit.py:488} INFO - 22/02/28 06:41:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-02-28 06:41:03,947] {spark_submit.py:488} INFO - 22/02/28 06:41:03 INFO ResourceUtils: ==============================================================
[2022-02-28 06:41:03,949] {spark_submit.py:488} INFO - 22/02/28 06:41:03 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-02-28 06:41:03,951] {spark_submit.py:488} INFO - 22/02/28 06:41:03 INFO ResourceUtils: ==============================================================
[2022-02-28 06:41:03,952] {spark_submit.py:488} INFO - 22/02/28 06:41:03 INFO SparkContext: Submitted application: test
[2022-02-28 06:41:03,988] {spark_submit.py:488} INFO - 22/02/28 06:41:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-02-28 06:41:04,007] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
[2022-02-28 06:41:04,010] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-02-28 06:41:04,078] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SecurityManager: Changing view acls to: root
[2022-02-28 06:41:04,079] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SecurityManager: Changing modify acls to: root
[2022-02-28 06:41:04,080] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SecurityManager: Changing view acls groups to:
[2022-02-28 06:41:04,080] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SecurityManager: Changing modify acls groups to:
[2022-02-28 06:41:04,081] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[2022-02-28 06:41:04,464] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO Utils: Successfully started service 'sparkDriver' on port 39413.
[2022-02-28 06:41:04,498] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SparkEnv: Registering MapOutputTracker
[2022-02-28 06:41:04,537] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SparkEnv: Registering BlockManagerMaster
[2022-02-28 06:41:04,575] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-02-28 06:41:04,577] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-02-28 06:41:04,584] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-02-28 06:41:04,618] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd521b4e-e053-4568-abfe-3b71ad5fa56b
[2022-02-28 06:41:04,647] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-02-28 06:41:04,673] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-02-28 06:41:04,970] {spark_submit.py:488} INFO - 22/02/28 06:41:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-02-28 06:41:05,044] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c5f5c4f93487:4040
[2022-02-28 06:41:05,324] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2022-02-28 06:41:05,415] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.3:7077 after 45 ms (0 ms spent in bootstraps)
[2022-02-28 06:41:05,592] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220228064105-0001
[2022-02-28 06:41:05,600] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/0 on worker-20220228063953-172.19.0.4-7000 (172.19.0.4:7000) with 2 core(s)
[2022-02-28 06:41:05,610] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/0 on hostPort 172.19.0.4:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:41:05,611] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/1 on worker-20220228063953-172.19.0.5-7000 (172.19.0.5:7000) with 2 core(s)
[2022-02-28 06:41:05,612] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/1 on hostPort 172.19.0.5:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:41:05,618] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37497.
[2022-02-28 06:41:05,619] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO NettyBlockTransferService: Server created on c5f5c4f93487:37497
[2022-02-28 06:41:05,621] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-02-28 06:41:05,645] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c5f5c4f93487, 37497, None)
[2022-02-28 06:41:05,667] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO BlockManagerMasterEndpoint: Registering block manager c5f5c4f93487:37497 with 434.4 MiB RAM, BlockManagerId(driver, c5f5c4f93487, 37497, None)
[2022-02-28 06:41:05,673] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c5f5c4f93487, 37497, None)
[2022-02-28 06:41:05,677] {spark_submit.py:488} INFO - 22/02/28 06:41:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c5f5c4f93487, 37497, None)
[2022-02-28 06:41:05,685] {spark_submit.py:488} INFO - 22/02/28 06:41:05 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:41:05,687] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:41:05,688] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:41:05,689] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:41:05,690] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:41:05,691] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:41:05,693] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:41:05,694] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:41:05,695] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:41:05,696] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:41:05,697] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:41:05,698] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:41:05,699] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:41:05,700] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:41:05,702] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:41:05,703] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:41:05,704] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:41:05,705] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:41:05,705] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:41:05,706] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:41:05,708] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:41:05,709] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:41:05,711] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:41:05,713] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:41:05,714] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:41:05,715] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,716] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,717] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,718] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:41:05,719] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,720] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,721] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,722] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:41:05,723] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,724] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,725] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,726] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:41:05,727] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,729] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,730] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,734] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:41:05,737] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,738] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,739] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:41:05,740] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:41:05,741] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:41:05,742] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:41:05,743] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:41:05,744] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:41:05,746] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:41:05,747] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:41:05,749] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:41:05,751] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:41:05,752] {spark_submit.py:488} INFO - 22/02/28 06:41:05 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:41:05,753] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:41:05,754] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:41:05,755] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:41:05,756] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:41:05,757] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:41:05,758] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:41:05,759] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:41:05,760] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:41:05,762] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:41:05,762] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:41:05,764] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:41:05,765] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:41:05,768] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:41:05,770] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:41:05,771] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:41:05,773] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:41:05,774] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:41:05,775] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:41:05,776] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:41:05,777] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:41:05,778] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:41:05,779] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:41:05,780] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:41:05,782] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:41:05,783] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,785] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,786] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,787] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:41:05,789] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,790] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,791] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,793] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:41:05,794] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,796] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,798] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,799] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:41:05,801] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,802] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,803] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:41:05,804] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:41:05,806] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:41:05,807] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:41:05,809] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:41:05,810] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:41:05,811] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:41:05,812] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:41:05,813] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:41:05,814] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:41:05,815] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:41:05,816] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:41:05,817] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:41:05,818] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:41:06,044] {spark_submit.py:488} INFO - 22/02/28 06:41:06 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2022-02-28 06:41:06,509] {spark_submit.py:488} INFO - 22/02/28 06:41:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-02-28 06:41:06,570] {spark_submit.py:488} INFO - 22/02/28 06:41:06 INFO SharedState: Warehouse path is 'file:/airflow/spark-warehouse'.
[2022-02-28 06:41:07,897] {spark_submit.py:488} INFO - /usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
[2022-02-28 06:41:07,911] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:41:07,913] {spark_submit.py:488} INFO - Step 3: Load Data
[2022-02-28 06:41:07,914] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:41:08,613] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166
[2022-02-28 06:41:08,649] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions
[2022-02-28 06:41:08,650] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)
[2022-02-28 06:41:08,652] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO DAGScheduler: Parents of final stage: List()
[2022-02-28 06:41:08,656] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO DAGScheduler: Missing parents: List()
[2022-02-28 06:41:08,671] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at RDD at PythonRDD.scala:53), which has no missing parents
[2022-02-28 06:41:08,911] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.4 KiB, free 434.4 MiB)
[2022-02-28 06:41:08,984] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
[2022-02-28 06:41:08,991] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c5f5c4f93487:37497 (size: 4.0 KiB, free: 434.4 MiB)
[2022-02-28 06:41:09,001] {spark_submit.py:488} INFO - 22/02/28 06:41:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
[2022-02-28 06:41:09,022] {spark_submit.py:488} INFO - 22/02/28 06:41:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[1] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
[2022-02-28 06:41:09,025] {spark_submit.py:488} INFO - 22/02/28 06:41:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-02-28 06:41:24,076] {spark_submit.py:488} INFO - 22/02/28 06:41:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:41:39,072] {spark_submit.py:488} INFO - 22/02/28 06:41:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:41:54,070] {spark_submit.py:488} INFO - 22/02/28 06:41:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:42:09,071] {spark_submit.py:488} INFO - 22/02/28 06:42:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:42:24,070] {spark_submit.py:488} INFO - 22/02/28 06:42:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:42:39,072] {spark_submit.py:488} INFO - 22/02/28 06:42:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:42:54,070] {spark_submit.py:488} INFO - 22/02/28 06:42:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:43:09,070] {spark_submit.py:488} INFO - 22/02/28 06:43:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:43:09,400] {spark_submit.py:488} INFO - 22/02/28 06:43:09 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:43:09,401] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:43:09,402] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:43:09,403] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:43:09,404] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:43:09,405] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:43:09,406] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:43:09,407] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:43:09,408] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:43:09,409] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:43:09,412] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:43:09,413] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:43:09,414] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,416] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:43:09,417] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,417] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,418] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,420] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:43:09,421] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:43:09,422] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:43:09,423] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:43:09,424] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:43:09,425] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:43:09,427] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:43:09,428] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:43:09,429] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,430] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,431] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,432] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:43:09,432] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,433] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,434] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,435] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:43:09,436] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,438] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,439] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,440] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:43:09,442] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,445] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,452] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,453] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:43:09,454] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,455] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,457] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:43:09,458] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:43:09,459] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:43:09,460] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:43:09,462] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:43:09,463] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:43:09,464] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:43:09,465] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:43:09,466] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:43:09,467] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:43:09,468] {spark_submit.py:488} INFO - 22/02/28 06:43:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/2 on worker-20220228063953-172.19.0.5-7000 (172.19.0.5:7000) with 2 core(s)
[2022-02-28 06:43:09,469] {spark_submit.py:488} INFO - 22/02/28 06:43:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/2 on hostPort 172.19.0.5:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:43:09,469] {spark_submit.py:488} INFO - 22/02/28 06:43:09 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:43:09,474] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:43:09,475] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:43:09,476] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:43:09,476] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:43:09,477] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:43:09,478] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:43:09,479] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:43:09,480] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:43:09,481] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:43:09,481] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:43:09,483] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:43:09,484] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,485] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:43:09,486] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,486] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,487] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,488] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:43:09,489] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:43:09,490] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:43:09,491] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:43:09,492] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:43:09,493] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:43:09,494] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:43:09,494] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:43:09,495] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,496] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,497] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,498] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:43:09,499] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,500] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,500] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,501] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:43:09,502] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,503] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,503] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,504] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:43:09,505] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,506] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,507] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,507] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:43:09,508] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,509] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,509] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:43:09,510] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:43:09,511] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:43:09,512] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:43:09,513] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:43:09,514] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:43:09,514] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:43:09,515] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:43:09,516] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:43:09,517] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:43:09,518] {spark_submit.py:488} INFO - 22/02/28 06:43:09 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/3 on worker-20220228063953-172.19.0.4-7000 (172.19.0.4:7000) with 2 core(s)
[2022-02-28 06:43:09,518] {spark_submit.py:488} INFO - 22/02/28 06:43:09 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/3 on hostPort 172.19.0.4:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:43:09,519] {spark_submit.py:488} INFO - 22/02/28 06:43:09 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:43:09,520] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:43:09,521] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:43:09,521] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:43:09,522] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:43:09,523] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:43:09,524] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:43:09,524] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:43:09,525] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:43:09,526] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:43:09,527] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:43:09,527] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:43:09,528] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,529] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:43:09,530] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,530] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,531] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,532] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:43:09,533] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:43:09,533] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:43:09,534] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:43:09,535] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:43:09,536] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:43:09,536] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:43:09,537] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:43:09,538] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,539] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,540] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,540] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:43:09,541] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,542] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,542] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,543] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:43:09,544] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,545] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,545] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,546] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:43:09,548] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,548] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,549] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,550] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:43:09,550] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,551] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,551] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:43:09,552] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:43:09,553] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:43:09,553] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:43:09,554] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:43:09,555] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:43:09,555] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:43:09,556] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:43:09,557] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:43:09,557] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:43:09,558] {spark_submit.py:488} INFO - 22/02/28 06:43:09 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:43:09,559] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:43:09,560] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:43:09,560] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:43:09,561] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:43:09,562] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:43:09,562] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:43:09,563] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:43:09,563] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:43:09,564] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:43:09,565] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:43:09,565] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:43:09,566] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,567] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:43:09,568] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,568] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:43:09,569] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:43:09,570] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:43:09,570] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:43:09,571] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:43:09,572] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:43:09,572] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:43:09,573] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:43:09,573] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:43:09,574] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:43:09,575] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,575] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,576] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,577] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:43:09,577] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,578] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,579] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,580] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:43:09,581] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,582] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,583] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,583] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:43:09,584] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,585] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,586] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:43:09,587] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:43:09,588] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:43:09,589] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:43:09,589] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:43:09,590] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:43:09,591] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:43:09,592] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:43:09,593] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:43:09,594] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:43:09,595] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:43:09,595] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:43:09,596] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:43:09,597] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:43:24,069] {spark_submit.py:488} INFO - 22/02/28 06:43:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:43:39,070] {spark_submit.py:488} INFO - 22/02/28 06:43:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:43:54,071] {spark_submit.py:488} INFO - 22/02/28 06:43:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:44:09,070] {spark_submit.py:488} INFO - 22/02/28 06:44:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:44:24,070] {spark_submit.py:488} INFO - 22/02/28 06:44:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:44:39,070] {spark_submit.py:488} INFO - 22/02/28 06:44:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:44:54,070] {spark_submit.py:488} INFO - 22/02/28 06:44:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:45:09,071] {spark_submit.py:488} INFO - 22/02/28 06:45:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:45:12,028] {spark_submit.py:488} INFO - 22/02/28 06:45:12 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:45:12,030] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:45:12,031] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:45:12,033] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:45:12,034] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:45:12,036] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:45:12,038] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:45:12,040] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:45:12,041] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:45:12,042] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:45:12,043] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:45:12,045] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:45:12,046] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,047] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:45:12,048] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,049] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,050] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,052] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:45:12,053] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:45:12,054] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:45:12,055] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:45:12,056] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:45:12,057] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:45:12,058] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:45:12,059] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:45:12,060] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,061] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,062] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,063] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:45:12,065] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,066] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,067] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,068] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:45:12,069] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,071] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,072] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,073] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:45:12,073] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,074] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,075] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,076] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:45:12,077] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,079] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,080] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:45:12,081] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:45:12,082] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:45:12,083] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:45:12,084] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:45:12,085] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:45:12,086] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:45:12,087] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:45:12,088] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:45:12,089] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:45:12,089] {spark_submit.py:488} INFO - 22/02/28 06:45:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/4 on worker-20220228063953-172.19.0.5-7000 (172.19.0.5:7000) with 2 core(s)
[2022-02-28 06:45:12,090] {spark_submit.py:488} INFO - 22/02/28 06:45:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/4 on hostPort 172.19.0.5:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:45:12,092] {spark_submit.py:488} INFO - 22/02/28 06:45:12 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:45:12,093] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:45:12,093] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:45:12,094] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:45:12,095] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:45:12,096] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:45:12,097] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:45:12,098] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:45:12,099] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:45:12,100] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:45:12,101] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:45:12,101] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:45:12,102] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,103] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:45:12,104] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,104] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,105] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,106] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:45:12,107] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:45:12,107] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:45:12,108] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:45:12,109] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:45:12,109] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:45:12,110] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:45:12,111] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:45:12,111] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,112] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,113] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,114] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:45:12,115] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,121] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,122] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,123] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:45:12,125] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,127] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,128] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,129] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:45:12,130] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,131] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,133] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,134] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:45:12,135] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,136] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,138] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:45:12,139] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:45:12,140] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:45:12,141] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:45:12,142] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:45:12,143] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:45:12,144] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:45:12,145] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:45:12,146] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:45:12,147] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:45:12,158] {spark_submit.py:488} INFO - 22/02/28 06:45:12 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:45:12,161] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:45:12,163] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:45:12,164] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:45:12,166] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:45:12,168] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:45:12,170] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:45:12,172] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:45:12,173] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:45:12,174] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:45:12,175] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:45:12,176] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:45:12,177] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,178] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:45:12,178] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,179] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,180] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,181] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:45:12,182] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:45:12,183] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:45:12,184] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:45:12,185] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:45:12,186] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:45:12,187] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:45:12,188] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:45:12,189] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,190] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,191] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,192] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:45:12,193] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,194] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,195] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,196] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:45:12,197] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,198] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,199] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,200] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:45:12,201] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,202] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,204] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,205] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:45:12,208] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,209] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,210] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:45:12,211] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:45:12,212] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:45:12,213] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:45:12,214] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:45:12,215] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:45:12,216] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:45:12,217] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:45:12,217] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:45:12,218] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:45:12,219] {spark_submit.py:488} INFO - 22/02/28 06:45:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/5 on worker-20220228063953-172.19.0.4-7000 (172.19.0.4:7000) with 2 core(s)
[2022-02-28 06:45:12,220] {spark_submit.py:488} INFO - 22/02/28 06:45:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/5 on hostPort 172.19.0.4:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:45:12,222] {spark_submit.py:488} INFO - 22/02/28 06:45:12 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:45:12,223] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:45:12,224] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:45:12,225] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:45:12,226] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:45:12,226] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:45:12,227] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:45:12,228] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:45:12,229] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:45:12,230] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:45:12,230] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:45:12,231] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:45:12,232] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,233] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:45:12,233] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,234] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:45:12,235] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:45:12,236] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:45:12,236] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:45:12,237] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:45:12,238] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:45:12,239] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:45:12,239] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:45:12,240] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:45:12,241] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:45:12,241] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,242] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,242] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,243] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:45:12,244] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,245] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,246] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,246] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:45:12,247] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,248] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,248] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,249] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:45:12,250] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,250] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,251] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:45:12,252] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:45:12,253] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:45:12,253] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:45:12,254] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:45:12,255] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:45:12,256] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:45:12,256] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:45:12,257] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:45:12,258] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:45:12,258] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:45:12,259] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:45:12,260] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:45:12,260] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:45:24,069] {spark_submit.py:488} INFO - 22/02/28 06:45:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:45:39,070] {spark_submit.py:488} INFO - 22/02/28 06:45:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:45:54,069] {spark_submit.py:488} INFO - 22/02/28 06:45:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:46:09,070] {spark_submit.py:488} INFO - 22/02/28 06:46:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:46:24,071] {spark_submit.py:488} INFO - 22/02/28 06:46:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:46:39,071] {spark_submit.py:488} INFO - 22/02/28 06:46:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:46:54,071] {spark_submit.py:488} INFO - 22/02/28 06:46:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:47:09,071] {spark_submit.py:488} INFO - 22/02/28 06:47:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:47:14,827] {spark_submit.py:488} INFO - 22/02/28 06:47:14 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:47:14,828] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:47:14,829] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:47:14,830] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:47:14,831] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:47:14,832] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:47:14,833] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:47:14,841] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:47:14,842] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:47:14,843] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:47:14,844] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:47:14,845] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:47:14,846] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:14,847] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:47:14,847] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:47:14,848] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:14,849] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:47:14,850] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:47:14,851] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:47:14,852] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:47:14,853] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:47:14,854] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:47:14,855] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:47:14,856] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:47:14,856] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:47:14,857] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,859] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,860] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,861] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:47:14,862] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,863] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,864] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,865] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:47:14,866] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,867] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,868] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,868] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:47:14,869] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,871] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,872] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,873] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:47:14,874] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,875] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,876] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:47:14,878] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:47:14,879] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:47:14,880] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:47:14,881] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:47:14,882] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:47:14,883] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:47:14,884] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:47:14,885] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:47:14,886] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:47:14,887] {spark_submit.py:488} INFO - 22/02/28 06:47:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/6 on worker-20220228063953-172.19.0.5-7000 (172.19.0.5:7000) with 2 core(s)
[2022-02-28 06:47:14,888] {spark_submit.py:488} INFO - 22/02/28 06:47:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/6 on hostPort 172.19.0.5:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:47:14,889] {spark_submit.py:488} INFO - 22/02/28 06:47:14 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:47:14,890] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:47:14,891] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:47:14,892] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:47:14,893] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:47:14,894] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:47:14,895] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:47:14,896] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:47:14,897] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:47:14,898] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:47:14,899] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:47:14,900] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:47:14,901] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:14,902] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:47:14,903] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:47:14,905] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:14,906] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:47:14,909] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:47:14,911] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:47:14,912] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:47:14,915] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:47:14,916] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:47:14,918] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:47:14,919] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:47:14,920] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:47:14,921] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,922] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,923] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,925] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:47:14,926] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,927] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,928] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,929] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:47:14,930] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,931] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,932] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,933] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:47:14,934] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,935] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,936] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,936] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:47:14,937] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,938] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,940] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:47:14,941] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:47:14,942] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:47:14,943] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:47:14,943] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:47:14,944] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:47:14,945] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:47:14,946] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:47:14,947] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:47:14,948] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:47:14,949] {spark_submit.py:488} INFO - 22/02/28 06:47:14 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:47:14,950] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:47:14,951] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:47:14,952] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:47:14,953] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:47:14,953] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:47:14,954] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:47:14,955] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:47:14,956] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:47:14,957] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:47:14,958] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:47:14,959] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:47:14,960] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:14,961] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:47:14,961] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:47:14,962] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:14,963] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:47:14,964] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:47:14,965] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:47:14,967] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:47:14,968] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:47:14,969] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:47:14,970] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:47:14,971] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:47:14,972] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:47:14,972] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,973] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,974] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,975] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:47:14,975] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,976] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,977] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,978] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:47:14,978] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,979] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,980] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,981] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:47:14,982] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,983] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,983] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:14,984] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:47:14,985] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:14,985] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:14,986] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:47:14,987] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:47:14,988] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:47:14,989] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:47:14,990] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:47:14,990] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:47:14,991] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:47:14,992] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:47:14,992] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:47:14,993] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:47:14,994] {spark_submit.py:488} INFO - 22/02/28 06:47:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064105-0001/7 on worker-20220228063953-172.19.0.4-7000 (172.19.0.4:7000) with 2 core(s)
[2022-02-28 06:47:14,994] {spark_submit.py:488} INFO - 22/02/28 06:47:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064105-0001/7 on hostPort 172.19.0.4:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:47:14,995] {spark_submit.py:488} INFO - 22/02/28 06:47:14 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:47:14,996] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:47:14,996] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:47:14,997] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:47:14,998] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:47:14,999] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:47:14,999] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:47:15,000] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:47:15,000] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:47:15,001] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:47:15,002] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:47:15,002] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:47:15,003] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:15,004] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:47:15,004] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:47:15,005] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:47:15,006] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:47:15,006] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:47:15,007] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:47:15,008] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:47:15,009] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:47:15,009] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:47:15,010] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:47:15,011] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:47:15,011] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:47:15,012] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:15,013] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:15,014] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:15,014] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:47:15,015] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:15,016] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:15,016] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:15,017] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:47:15,018] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:15,018] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:15,019] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:15,020] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:47:15,020] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:15,021] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:15,022] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:47:15,022] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:47:15,023] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:47:15,024] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:47:15,024] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:47:15,026] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:47:15,027] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:47:15,027] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:47:15,028] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:47:15,029] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:47:15,030] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:47:15,031] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:47:15,031] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:47:15,032] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:47:24,070] {spark_submit.py:488} INFO - 22/02/28 06:47:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:47:39,070] {spark_submit.py:488} INFO - 22/02/28 06:47:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:47:54,070] {spark_submit.py:488} INFO - 22/02/28 06:47:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:48:09,069] {spark_submit.py:488} INFO - 22/02/28 06:48:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:48:24,069] {spark_submit.py:488} INFO - 22/02/28 06:48:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:48:39,070] {spark_submit.py:488} INFO - 22/02/28 06:48:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:48:54,071] {spark_submit.py:488} INFO - 22/02/28 06:48:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:48:58,442] {spark_submit.py:488} INFO - 22/02/28 06:48:58 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED
[2022-02-28 06:48:58,446] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-02-28 06:48:58,464] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO TaskSchedulerImpl: Cancelling stage 0
[2022-02-28 06:48:58,465] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
[2022-02-28 06:48:58,469] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) failed in 469.762 s due to Job aborted due to stage failure: Master removed our application: KILLED
[2022-02-28 06:48:58,479] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO DAGScheduler: Job 0 failed: runJob at PythonRDD.scala:166, took 470.422127 s
[2022-02-28 06:48:58,491] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO SparkUI: Stopped Spark web UI at http://c5f5c4f93487:4040
[2022-02-28 06:48:58,502] {spark_submit.py:488} INFO - Traceback (most recent call last):
[2022-02-28 06:48:58,505] {spark_submit.py:488} INFO - File "/sparks/production/0747/document_etl.py", line 49, in <module>
[2022-02-28 06:48:58,508] {spark_submit.py:488} INFO - df = rdd.toDF(columns)
[2022-02-28 06:48:58,509] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 66, in toDF
[2022-02-28 06:48:58,512] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO StandaloneSchedulerBackend: Shutting down all executors
[2022-02-28 06:48:58,514] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 675, in createDataFrame
[2022-02-28 06:48:58,515] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
[2022-02-28 06:48:58,519] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 698, in _create_dataframe
[2022-02-28 06:48:58,521] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 486, in _createFromRDD
[2022-02-28 06:48:58,523] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 460, in _inferSchema
[2022-02-28 06:48:58,524] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 1588, in first
[2022-02-28 06:48:58,526] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 1568, in take
[2022-02-28 06:48:58,529] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 1227, in runJob
[2022-02-28 06:48:58,531] {spark_submit.py:488} INFO - 22/02/28 06:48:58 ERROR Utils: Uncaught exception in thread stop-spark-context
[2022-02-28 06:48:58,533] {spark_submit.py:488} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2022-02-28 06:48:58,535] {spark_submit.py:488} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2022-02-28 06:48:58,536] {spark_submit.py:488} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2022-02-28 06:48:58,538] {spark_submit.py:488} INFO - at org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:287)
[2022-02-28 06:48:58,541] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:259)
[2022-02-28 06:48:58,543] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:131)
[2022-02-28 06:48:58,544] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:927)
[2022-02-28 06:48:58,546] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2567)
[2022-02-28 06:48:58,547] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2086)
[2022-02-28 06:48:58,548] {spark_submit.py:488} INFO - at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)
[2022-02-28 06:48:58,550] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.stop(SparkContext.scala:2086)
[2022-02-28 06:48:58,551] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2035)
[2022-02-28 06:48:58,553] {spark_submit.py:488} INFO - Caused by: org.apache.spark.SparkException: Could not find AppClient.
[2022-02-28 06:48:58,555] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)
[2022-02-28 06:48:58,557] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
[2022-02-28 06:48:58,559] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
[2022-02-28 06:48:58,562] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
[2022-02-28 06:48:58,563] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
[2022-02-28 06:48:58,564] {spark_submit.py:488} INFO - at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:74)
[2022-02-28 06:48:58,566] {spark_submit.py:488} INFO - ... 9 more
[2022-02-28 06:48:58,567] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2022-02-28 06:48:58,568] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
[2022-02-28 06:48:58,570] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 328, in get_return_value
[2022-02-28 06:48:58,572] {spark_submit.py:488} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
[2022-02-28 06:48:58,573] {spark_submit.py:488} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED
[2022-02-28 06:48:58,574] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
[2022-02-28 06:48:58,575] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
[2022-02-28 06:48:58,576] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
[2022-02-28 06:48:58,578] {spark_submit.py:488} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2022-02-28 06:48:58,581] {spark_submit.py:488} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2022-02-28 06:48:58,582] {spark_submit.py:488} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2022-02-28 06:48:58,583] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
[2022-02-28 06:48:58,585] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
[2022-02-28 06:48:58,586] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
[2022-02-28 06:48:58,587] {spark_submit.py:488} INFO - at scala.Option.foreach(Option.scala:407)
[2022-02-28 06:48:58,589] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
[2022-02-28 06:48:58,590] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
[2022-02-28 06:48:58,591] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
[2022-02-28 06:48:58,592] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
[2022-02-28 06:48:58,593] {spark_submit.py:488} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2022-02-28 06:48:58,594] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
[2022-02-28 06:48:58,595] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
[2022-02-28 06:48:58,596] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
[2022-02-28 06:48:58,597] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
[2022-02-28 06:48:58,600] {spark_submit.py:488} INFO - at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)
[2022-02-28 06:48:58,601] {spark_submit.py:488} INFO - at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
[2022-02-28 06:48:58,602] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-28 06:48:58,603] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-28 06:48:58,605] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-28 06:48:58,606] {spark_submit.py:488} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-28 06:48:58,608] {spark_submit.py:488} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-02-28 06:48:58,609] {spark_submit.py:488} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-02-28 06:48:58,610] {spark_submit.py:488} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2022-02-28 06:48:58,612] {spark_submit.py:488} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-02-28 06:48:58,613] {spark_submit.py:488} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-02-28 06:48:58,614] {spark_submit.py:488} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2022-02-28 06:48:58,616] {spark_submit.py:488} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2022-02-28 06:48:58,617] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:48:58,618] {spark_submit.py:488} INFO - 
[2022-02-28 06:48:58,620] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-02-28 06:48:58,622] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO ShutdownHookManager: Shutdown hook called
[2022-02-28 06:48:58,623] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-e041f418-d6ad-46e5-8503-aa92b34976e5
[2022-02-28 06:48:58,625] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO MemoryStore: MemoryStore cleared
[2022-02-28 06:48:58,627] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO BlockManager: BlockManager stopped
[2022-02-28 06:48:58,628] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b73ac7f-975d-4f73-bb8c-605a89c4323f/pyspark-994f186b-8ea5-461a-8d75-82a99bc8f7e7
[2022-02-28 06:48:58,634] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b73ac7f-975d-4f73-bb8c-605a89c4323f
[2022-02-28 06:48:58,642] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-2b73ac7f-975d-4f73-bb8c-605a89c4323f/userFiles-22c4b0aa-711e-4191-82ae-718ff486f350
[2022-02-28 06:48:58,656] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-02-28 06:48:58,672] {spark_submit.py:488} INFO - 22/02/28 06:48:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-02-28 06:48:58,973] {taskinstance.py:1718} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1334, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1460, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1516, in _execute_task
    result = execute_callable(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 420, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --name 0747 Pipeline --queue default /sparks/production/0747/document_etl.py. Error code is: 1.
[2022-02-28 06:48:58,987] {taskinstance.py:1282} INFO - Marking task as FAILED. dag_id=0747_pipeline, task_id=document_etl, execution_date=20220228T063706, start_date=20220228T064101, end_date=20220228T064858
[2022-02-28 06:48:59,066] {standard_task_runner.py:92} ERROR - Failed to execute job 73 for task document_etl
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1334, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1460, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1516, in _execute_task
    result = execute_callable(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 420, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --name 0747 Pipeline --queue default /sparks/production/0747/document_etl.py. Error code is: 1.
[2022-02-28 06:48:59,083] {local_task_job.py:154} INFO - Task exited with return code 1
[2022-02-28 06:48:59,170] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
