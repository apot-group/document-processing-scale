[2022-02-28 06:39:59,617] {taskinstance.py:1037} INFO - Dependencies all met for <TaskInstance: 0747_pipeline.document_etl manual__2022-02-28T06:37:06.521999+00:00 [queued]>
[2022-02-28 06:39:59,633] {taskinstance.py:1037} INFO - Dependencies all met for <TaskInstance: 0747_pipeline.document_etl manual__2022-02-28T06:37:06.521999+00:00 [queued]>
[2022-02-28 06:39:59,634] {taskinstance.py:1243} INFO - 
--------------------------------------------------------------------------------
[2022-02-28 06:39:59,635] {taskinstance.py:1244} INFO - Starting attempt 2 of 1
[2022-02-28 06:39:59,636] {taskinstance.py:1245} INFO - 
--------------------------------------------------------------------------------
[2022-02-28 06:39:59,688] {taskinstance.py:1264} INFO - Executing <Task(SparkSubmitOperator): document_etl> on 2022-02-28 06:37:06.521999+00:00
[2022-02-28 06:39:59,690] {standard_task_runner.py:52} INFO - Started process 31 to run task
[2022-02-28 06:39:59,693] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', '0747_pipeline', 'document_etl', 'manual__2022-02-28T06:37:06.521999+00:00', '--job-id', '72', '--raw', '--subdir', 'DAGS_FOLDER/0747.py', '--cfg-path', '/tmp/tmp1sa_y2mt', '--error-file', '/tmp/tmp94r8yivu']
[2022-02-28 06:39:59,695] {standard_task_runner.py:77} INFO - Job 72: Subtask document_etl
[2022-02-28 06:39:59,779] {logging_mixin.py:109} INFO - Running <TaskInstance: 0747_pipeline.document_etl manual__2022-02-28T06:37:06.521999+00:00 [running]> on host c5f5c4f93487
[2022-02-28 06:39:59,875] {taskinstance.py:1431} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=0747_pipeline
AIRFLOW_CTX_TASK_ID=document_etl
AIRFLOW_CTX_EXECUTION_DATE=2022-02-28T06:37:06.521999+00:00
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-02-28T06:37:06.521999+00:00
[2022-02-28 06:39:59,885] {base.py:79} INFO - Using connection to: id: spark_default. Host: spark://spark-master, Port: 7077, Schema: , Login: , Password: None, extra: {'queue': 'default', 'spark-home': '', 'spark-binary': 'spark-submit', 'namespace': 'default'}
[2022-02-28 06:39:59,887] {spark_submit.py:335} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --name 0747 Pipeline --queue default /sparks/production/0747/document_etl.py
[2022-02-28 06:40:01,396] {spark_submit.py:488} INFO - WARNING: An illegal reflective access operation has occurred
[2022-02-28 06:40:01,398] {spark_submit.py:488} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2022-02-28 06:40:01,399] {spark_submit.py:488} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2022-02-28 06:40:01,400] {spark_submit.py:488} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2022-02-28 06:40:01,400] {spark_submit.py:488} INFO - WARNING: All illegal access operations will be denied in a future release
[2022-02-28 06:40:01,996] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:40:01,997] {spark_submit.py:488} INFO - Step 1: Import Libraries
[2022-02-28 06:40:01,998] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:40:02,250] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:40:02,251] {spark_submit.py:488} INFO - Step 2: Start Spark Session
[2022-02-28 06:40:02,252] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:40:02,350] {spark_submit.py:488} INFO - Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
[2022-02-28 06:40:02,362] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO SparkContext: Running Spark version 3.2.1
[2022-02-28 06:40:02,483] {spark_submit.py:488} INFO - 22/02/28 06:40:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2022-02-28 06:40:02,666] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO ResourceUtils: ==============================================================
[2022-02-28 06:40:02,667] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2022-02-28 06:40:02,668] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO ResourceUtils: ==============================================================
[2022-02-28 06:40:02,669] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO SparkContext: Submitted application: test
[2022-02-28 06:40:02,710] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2022-02-28 06:40:02,729] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
[2022-02-28 06:40:02,734] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2022-02-28 06:40:02,825] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO SecurityManager: Changing view acls to: root
[2022-02-28 06:40:02,826] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO SecurityManager: Changing modify acls to: root
[2022-02-28 06:40:02,827] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO SecurityManager: Changing view acls groups to:
[2022-02-28 06:40:02,828] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO SecurityManager: Changing modify acls groups to:
[2022-02-28 06:40:02,829] {spark_submit.py:488} INFO - 22/02/28 06:40:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
[2022-02-28 06:40:03,359] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO Utils: Successfully started service 'sparkDriver' on port 41007.
[2022-02-28 06:40:03,407] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO SparkEnv: Registering MapOutputTracker
[2022-02-28 06:40:03,461] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO SparkEnv: Registering BlockManagerMaster
[2022-02-28 06:40:03,499] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2022-02-28 06:40:03,500] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2022-02-28 06:40:03,508] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2022-02-28 06:40:03,544] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-45d57aef-6980-47c8-8459-80f716b90856
[2022-02-28 06:40:03,576] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2022-02-28 06:40:03,602] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2022-02-28 06:40:03,927] {spark_submit.py:488} INFO - 22/02/28 06:40:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2022-02-28 06:40:04,031] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c5f5c4f93487:4040
[2022-02-28 06:40:04,390] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2022-02-28 06:40:04,545] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.3:7077 after 100 ms (0 ms spent in bootstraps)
[2022-02-28 06:40:04,794] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20220228064004-0000
[2022-02-28 06:40:04,824] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37545.
[2022-02-28 06:40:04,829] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO NettyBlockTransferService: Server created on c5f5c4f93487:37545
[2022-02-28 06:40:04,830] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2022-02-28 06:40:04,854] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c5f5c4f93487, 37545, None)
[2022-02-28 06:40:04,872] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO BlockManagerMasterEndpoint: Registering block manager c5f5c4f93487:37545 with 434.4 MiB RAM, BlockManagerId(driver, c5f5c4f93487, 37545, None)
[2022-02-28 06:40:04,878] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c5f5c4f93487, 37545, None)
[2022-02-28 06:40:04,886] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064004-0000/0 on worker-20220228063953-172.19.0.4-7000 (172.19.0.4:7000) with 2 core(s)
[2022-02-28 06:40:04,889] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c5f5c4f93487, 37545, None)
[2022-02-28 06:40:04,913] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064004-0000/0 on hostPort 172.19.0.4:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:40:04,917] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20220228064004-0000/1 on worker-20220228063953-172.19.0.5-7000 (172.19.0.5:7000) with 2 core(s)
[2022-02-28 06:40:04,918] {spark_submit.py:488} INFO - 22/02/28 06:40:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20220228064004-0000/1 on hostPort 172.19.0.5:7000 with 2 core(s), 2.0 GiB RAM
[2022-02-28 06:40:05,288] {spark_submit.py:488} INFO - 22/02/28 06:40:05 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:40:05,290] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:40:05,291] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:40:05,292] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:40:05,293] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:40:05,294] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:40:05,296] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:40:05,297] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:40:05,298] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:40:05,299] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:40:05,300] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:40:05,301] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:40:05,302] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:40:05,303] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:40:05,305] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:40:05,306] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:40:05,307] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:40:05,308] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:40:05,309] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:40:05,310] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:40:05,311] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:40:05,312] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:40:05,312] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:40:05,313] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:40:05,314] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:40:05,315] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,323] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,324] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,325] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:40:05,326] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,327] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,328] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,329] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:40:05,330] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,331] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,331] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,332] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:40:05,333] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,334] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,337] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,338] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:40:05,339] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,340] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,341] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:40:05,342] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:40:05,342] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:40:05,343] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:40:05,344] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:40:05,345] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:40:05,346] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:40:05,347] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:40:05,347] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:40:05,349] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:40:05,350] {spark_submit.py:488} INFO - 22/02/28 06:40:05 ERROR TransportRequestHandler: Error while invoking RpcHandler#receive() for one-way message.
[2022-02-28 06:40:05,350] {spark_submit.py:488} INFO - java.io.InvalidClassException: org.apache.spark.deploy.DeployMessages$ExecutorUpdated; local class incompatible: stream classdesc serialVersionUID = 1654279024112373855, local class serialVersionUID = -1971851081955655249
[2022-02-28 06:40:05,351] {spark_submit.py:488} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:689)
[2022-02-28 06:40:05,352] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:2014)
[2022-02-28 06:40:05,353] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1864)
[2022-02-28 06:40:05,354] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2195)
[2022-02-28 06:40:05,355] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1681)
[2022-02-28 06:40:05,356] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)
[2022-02-28 06:40:05,356] {spark_submit.py:488} INFO - at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)
[2022-02-28 06:40:05,357] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
[2022-02-28 06:40:05,358] {spark_submit.py:488} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:109)
[2022-02-28 06:40:05,360] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$2(NettyRpcEnv.scala:299)
[2022-02-28 06:40:05,361] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:40:05,363] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:352)
[2022-02-28 06:40:05,364] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$deserialize$1(NettyRpcEnv.scala:298)
[2022-02-28 06:40:05,365] {spark_submit.py:488} INFO - at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
[2022-02-28 06:40:05,367] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.deserialize(NettyRpcEnv.scala:298)
[2022-02-28 06:40:05,368] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:647)
[2022-02-28 06:40:05,369] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)
[2022-02-28 06:40:05,370] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:690)
[2022-02-28 06:40:05,371] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:274)
[2022-02-28 06:40:05,372] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:111)
[2022-02-28 06:40:05,372] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)
[2022-02-28 06:40:05,373] {spark_submit.py:488} INFO - at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
[2022-02-28 06:40:05,374] {spark_submit.py:488} INFO - at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
[2022-02-28 06:40:05,375] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,376] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,377] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,378] {spark_submit.py:488} INFO - at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
[2022-02-28 06:40:05,379] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,380] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,381] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,382] {spark_submit.py:488} INFO - at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
[2022-02-28 06:40:05,383] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,384] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,385] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,386] {spark_submit.py:488} INFO - at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
[2022-02-28 06:40:05,387] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,388] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,389] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
[2022-02-28 06:40:05,390] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
[2022-02-28 06:40:05,391] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
[2022-02-28 06:40:05,392] {spark_submit.py:488} INFO - at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
[2022-02-28 06:40:05,393] {spark_submit.py:488} INFO - at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
[2022-02-28 06:40:05,394] {spark_submit.py:488} INFO - at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
[2022-02-28 06:40:05,395] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
[2022-02-28 06:40:05,396] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
[2022-02-28 06:40:05,396] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
[2022-02-28 06:40:05,397] {spark_submit.py:488} INFO - at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
[2022-02-28 06:40:05,400] {spark_submit.py:488} INFO - at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
[2022-02-28 06:40:05,402] {spark_submit.py:488} INFO - at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
[2022-02-28 06:40:05,403] {spark_submit.py:488} INFO - at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[2022-02-28 06:40:05,405] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:40:05,657] {spark_submit.py:488} INFO - 22/02/28 06:40:05 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2022-02-28 06:40:06,303] {spark_submit.py:488} INFO - 22/02/28 06:40:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2022-02-28 06:40:06,353] {spark_submit.py:488} INFO - 22/02/28 06:40:06 INFO SharedState: Warehouse path is 'file:/airflow/spark-warehouse'.
[2022-02-28 06:40:07,999] {spark_submit.py:488} INFO - /usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.
[2022-02-28 06:40:08,007] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:40:08,008] {spark_submit.py:488} INFO - Step 3: Load Data
[2022-02-28 06:40:08,009] {spark_submit.py:488} INFO - ===========================
[2022-02-28 06:40:08,946] {spark_submit.py:488} INFO - 22/02/28 06:40:08 INFO SparkContext: Starting job: runJob at PythonRDD.scala:166
[2022-02-28 06:40:09,010] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO DAGScheduler: Got job 0 (runJob at PythonRDD.scala:166) with 1 output partitions
[2022-02-28 06:40:09,011] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:166)
[2022-02-28 06:40:09,012] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO DAGScheduler: Parents of final stage: List()
[2022-02-28 06:40:09,015] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO DAGScheduler: Missing parents: List()
[2022-02-28 06:40:09,026] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at RDD at PythonRDD.scala:53), which has no missing parents
[2022-02-28 06:40:09,353] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.4 KiB, free 434.4 MiB)
[2022-02-28 06:40:09,431] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 434.4 MiB)
[2022-02-28 06:40:09,438] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c5f5c4f93487:37545 (size: 4.0 KiB, free: 434.4 MiB)
[2022-02-28 06:40:09,448] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478
[2022-02-28 06:40:09,473] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[1] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))
[2022-02-28 06:40:09,479] {spark_submit.py:488} INFO - 22/02/28 06:40:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2022-02-28 06:40:24,523] {spark_submit.py:488} INFO - 22/02/28 06:40:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:40:39,520] {spark_submit.py:488} INFO - 22/02/28 06:40:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
[2022-02-28 06:40:41,826] {spark_submit.py:488} INFO - 22/02/28 06:40:41 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED
[2022-02-28 06:40:41,838] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2022-02-28 06:40:41,854] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO TaskSchedulerImpl: Cancelling stage 0
[2022-02-28 06:40:41,856] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
[2022-02-28 06:40:41,860] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:166) failed in 32.796 s due to Job aborted due to stage failure: Master removed our application: KILLED
[2022-02-28 06:40:41,871] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO DAGScheduler: Job 0 failed: runJob at PythonRDD.scala:166, took 32.956953 s
[2022-02-28 06:40:41,890] {spark_submit.py:488} INFO - Traceback (most recent call last):
[2022-02-28 06:40:41,893] {spark_submit.py:488} INFO - File "/sparks/production/0747/document_etl.py", line 49, in <module>
[2022-02-28 06:40:41,896] {spark_submit.py:488} INFO - df = rdd.toDF(columns)
[2022-02-28 06:40:41,900] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 66, in toDF
[2022-02-28 06:40:41,902] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO SparkUI: Stopped Spark web UI at http://c5f5c4f93487:4040
[2022-02-28 06:40:41,905] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 675, in createDataFrame
[2022-02-28 06:40:41,907] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 698, in _create_dataframe
[2022-02-28 06:40:41,908] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 486, in _createFromRDD
[2022-02-28 06:40:41,910] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/session.py", line 460, in _inferSchema
[2022-02-28 06:40:41,911] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 1588, in first
[2022-02-28 06:40:41,914] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/rdd.py", line 1568, in take
[2022-02-28 06:40:41,915] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/context.py", line 1227, in runJob
[2022-02-28 06:40:41,917] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2022-02-28 06:40:41,920] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO StandaloneSchedulerBackend: Shutting down all executors
[2022-02-28 06:40:41,923] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
[2022-02-28 06:40:41,925] {spark_submit.py:488} INFO - File "/usr/local/lib/python3.7/site-packages/pyspark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 328, in get_return_value
[2022-02-28 06:40:41,927] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
[2022-02-28 06:40:41,928] {spark_submit.py:488} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
[2022-02-28 06:40:41,930] {spark_submit.py:488} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: KILLED
[2022-02-28 06:40:41,931] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)
[2022-02-28 06:40:41,933] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)
[2022-02-28 06:40:41,935] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)
[2022-02-28 06:40:41,937] {spark_submit.py:488} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2022-02-28 06:40:41,939] {spark_submit.py:488} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2022-02-28 06:40:41,941] {spark_submit.py:488} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2022-02-28 06:40:41,943] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)
[2022-02-28 06:40:41,944] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)
[2022-02-28 06:40:41,948] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)
[2022-02-28 06:40:41,950] {spark_submit.py:488} INFO - at scala.Option.foreach(Option.scala:407)
[2022-02-28 06:40:41,951] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)
[2022-02-28 06:40:41,953] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)
[2022-02-28 06:40:41,954] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)
[2022-02-28 06:40:41,956] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)
[2022-02-28 06:40:41,958] {spark_submit.py:488} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2022-02-28 06:40:41,961] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)
[2022-02-28 06:40:41,963] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
[2022-02-28 06:40:41,965] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)
[2022-02-28 06:40:41,967] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)
[2022-02-28 06:40:41,969] {spark_submit.py:488} INFO - at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)
[2022-02-28 06:40:41,970] {spark_submit.py:488} INFO - at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
[2022-02-28 06:40:41,972] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2022-02-28 06:40:41,973] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2022-02-28 06:40:41,976] {spark_submit.py:488} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2022-02-28 06:40:41,979] {spark_submit.py:488} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2022-02-28 06:40:41,983] {spark_submit.py:488} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2022-02-28 06:40:41,984] {spark_submit.py:488} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2022-02-28 06:40:41,986] {spark_submit.py:488} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2022-02-28 06:40:41,987] {spark_submit.py:488} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2022-02-28 06:40:41,988] {spark_submit.py:488} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2022-02-28 06:40:41,990] {spark_submit.py:488} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2022-02-28 06:40:41,991] {spark_submit.py:488} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2022-02-28 06:40:41,992] {spark_submit.py:488} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2022-02-28 06:40:41,994] {spark_submit.py:488} INFO - 
[2022-02-28 06:40:41,995] {spark_submit.py:488} INFO - 22/02/28 06:40:41 ERROR Utils: Uncaught exception in thread stop-spark-context
[2022-02-28 06:40:41,996] {spark_submit.py:488} INFO - org.apache.spark.SparkException: Exception thrown in awaitResult:
[2022-02-28 06:40:41,997] {spark_submit.py:488} INFO - at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
[2022-02-28 06:40:41,999] {spark_submit.py:488} INFO - at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
[2022-02-28 06:40:42,001] {spark_submit.py:488} INFO - at org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:287)
[2022-02-28 06:40:42,003] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:259)
[2022-02-28 06:40:42,005] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:131)
[2022-02-28 06:40:42,006] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:927)
[2022-02-28 06:40:42,009] {spark_submit.py:488} INFO - at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2567)
[2022-02-28 06:40:42,010] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2086)
[2022-02-28 06:40:42,012] {spark_submit.py:488} INFO - at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1442)
[2022-02-28 06:40:42,013] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext.stop(SparkContext.scala:2086)
[2022-02-28 06:40:42,014] {spark_submit.py:488} INFO - at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2035)
[2022-02-28 06:40:42,016] {spark_submit.py:488} INFO - Caused by: org.apache.spark.SparkException: Could not find AppClient.
[2022-02-28 06:40:42,017] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)
[2022-02-28 06:40:42,019] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
[2022-02-28 06:40:42,020] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
[2022-02-28 06:40:42,021] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
[2022-02-28 06:40:42,023] {spark_submit.py:488} INFO - at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
[2022-02-28 06:40:42,024] {spark_submit.py:488} INFO - at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:74)
[2022-02-28 06:40:42,025] {spark_submit.py:488} INFO - ... 9 more
[2022-02-28 06:40:42,026] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2022-02-28 06:40:42,028] {spark_submit.py:488} INFO - 22/02/28 06:40:41 INFO ShutdownHookManager: Shutdown hook called
[2022-02-28 06:40:42,030] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO MemoryStore: MemoryStore cleared
[2022-02-28 06:40:42,032] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-9ac9dd4d-e5a6-4959-9e1b-c454b94420ac
[2022-02-28 06:40:42,034] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO BlockManager: BlockManager stopped
[2022-02-28 06:40:42,036] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-107a6761-f8a1-4a8d-a841-d025930f7cef/pyspark-c11f5e40-e983-4c04-adf9-178464804f77
[2022-02-28 06:40:42,040] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-107a6761-f8a1-4a8d-a841-d025930f7cef
[2022-02-28 06:40:42,042] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO ShutdownHookManager: Deleting directory /tmp/spark-107a6761-f8a1-4a8d-a841-d025930f7cef/userFiles-5aead027-a520-4265-a6dd-6df2a545333e
[2022-02-28 06:40:42,044] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO BlockManagerMaster: BlockManagerMaster stopped
[2022-02-28 06:40:42,049] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2022-02-28 06:40:42,114] {spark_submit.py:488} INFO - 22/02/28 06:40:42 INFO SparkContext: Successfully stopped SparkContext
[2022-02-28 06:40:42,248] {taskinstance.py:1718} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1334, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1460, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1516, in _execute_task
    result = execute_callable(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 420, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --name 0747 Pipeline --queue default /sparks/production/0747/document_etl.py. Error code is: 1.
[2022-02-28 06:40:42,315] {taskinstance.py:1282} INFO - Marking task as FAILED. dag_id=0747_pipeline, task_id=document_etl, execution_date=20220228T063706, start_date=20220228T063959, end_date=20220228T064042
[2022-02-28 06:40:42,430] {standard_task_runner.py:92} ERROR - Failed to execute job 72 for task document_etl
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/usr/local/lib/python3.7/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/usr/local/lib/python3.7/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1334, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1460, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 1516, in _execute_task
    result = execute_callable(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/usr/local/lib/python3.7/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 420, in submit
    f"Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}."
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --name 0747 Pipeline --queue default /sparks/production/0747/document_etl.py. Error code is: 1.
[2022-02-28 06:40:42,442] {local_task_job.py:154} INFO - Task exited with return code 1
[2022-02-28 06:40:42,548] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
